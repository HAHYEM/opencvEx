{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# opencv를 위한 머신러닝\n",
    "\n",
    "딥러닝과 머신러닝의 차이점\n",
    "\n",
    "인공 지능이 가장 큰 원이고, 그 다음이 머신 러닝이며, 현재의 인공지능 붐을 주도하는 딥 러닝이 가장 작은 원이라 할 수 있다.\n",
    "머신 러닝: 인공 지능을 구현하는 구체적 접근 방식\n",
    "딥 러닝: 완전한 머신 러닝을 실현하는 기술"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'face_recognition'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-edc43d17263f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Load the jpg file into a numpy array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_image_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"face.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'face_recognition'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import face_recognition\n",
    "\n",
    "# Load the jpg file into a numpy array\n",
    "image = face_recognition.load_image_file(\"face.jpg\")\n",
    "\n",
    "# Find all the faces in the image using the default HOG-based model.\n",
    "# This method is fairly accurate, but not as accurate as the CNN model and not GPU accelerated.\n",
    "# See also: find_faces_in_picture_cnn.py\n",
    "face_locations = face_recognition.face_locations(image)\n",
    "\n",
    "print(\"I found {} face(s) in this photograph.\".format(len(face_locations)))\n",
    "\n",
    "for face_location in face_locations:\n",
    "\n",
    "    # Print the location of each face in this image\n",
    "    top, right, bottom, left = face_location\n",
    "    print(\"A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n",
    "\n",
    "    # You can access the actual face itself like this:\n",
    "    face_image = image[top:bottom, left:right]\n",
    "    pil_image = Image.fromarray(face_image)\n",
    "    pil_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "import sqlite3\n",
    "import os\n",
    "conn = sqlite3.connect('database.db')\n",
    "if not os.path.exists('./dataset'):\n",
    "    os.makedirs('./dataset')\n",
    "c = conn.cursor()\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "cap = cv2.VideoCapture(0)\n",
    "uname = input(\"Enter your name: \")\n",
    "c.execute('INSERT INTO users (name) VALUES (?)', (uname,))\n",
    "uid = c.lastrowid\n",
    "sampleNum = 0\n",
    "while True:\n",
    "  ret, img = cap.read()\n",
    "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "  faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "  for (x,y,w,h) in faces:\n",
    "    sampleNum = sampleNum+1\n",
    "    cv2.imwrite(\"dataset/User.\"+str(uid)+\".\"+str(sampleNum)+\".jpg\",gray[y:y+h,x:x+w])\n",
    "    cv2.rectangle(img, (x,y), (x+w, y+h), (255,0,0), 2)\n",
    "    cv2.waitKey(100)\n",
    "  cv2.imshow('img',img)\n",
    "  cv2.waitKey(1);\n",
    "  if sampleNum > 20:\n",
    "    break\n",
    "cap.release()\n",
    "conn.commit()\n",
    "conn.close()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np \n",
    "from PIL import Image\n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "path = 'dataset'\n",
    "if not os.path.exists('./recognizer'):\n",
    "    os.makedirs('./recognizer')\n",
    "def getImagesWithID(path):\n",
    "    imagePaths = [os.path.join(path,f) for f in os.listdir(path)]\n",
    "    faces = []\n",
    "    IDs = []\n",
    "    for imagePath in imagePaths:\n",
    "        faceImg = Image.open(imagePath).convert('L')\n",
    "        faceNp = np.array(faceImg,'uint8')\n",
    "        ID = int(os.path.split(imagePath)[-1].split('.')[1])\n",
    "        faces.append(faceNp)\n",
    "        IDs.append(ID)\n",
    "        cv2.imshow(\"training\",faceNp)\n",
    "        cv2.waitKey(10)\n",
    "    return np.array(IDs), faces\n",
    "Ids, faces = getImagesWithID(path)\n",
    "recognizer.train(faces,Ids)\n",
    "#recognizer.save('recognizer/trainingData.yml')\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'face_recognition'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-83642b857f46>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[1;32mimport\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;31m# Load the jpg file into a numpy array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mface_recognition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_image_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"face.jpg\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'face_recognition'"
     ]
    }
   ],
   "source": [
    "# face_recognition을 이용해서 하는 건데.. 그런 모듈이 없다고 나옵니닿ㅎ\n",
    "\n",
    "from PIL import Image\n",
    "import face_recognition\n",
    "\n",
    "# Load the jpg file into a numpy array\n",
    "image = face_recognition.load_image_file(\"face.jpg\")\n",
    "\n",
    "# Find all the faces in the image using the default HOG-based model.\n",
    "# This method is fairly accurate, but not as accurate as the CNN model and not GPU accelerated.\n",
    "# See also: find_faces_in_picture_cnn.py\n",
    "face_locations = face_recognition.face_locations(image)\n",
    "\n",
    "print(\"I found {} face(s) in this photograph.\".format(len(face_locations)))\n",
    "\n",
    "for face_location in face_locations:\n",
    "\n",
    "    # Print the location of each face in this image\n",
    "    top, right, bottom, left = face_location\n",
    "    print(\"A face is located at pixel location Top: {}, Left: {}, Bottom: {}, Right: {}\".format(top, left, bottom, right))\n",
    "\n",
    "    # You can access the actual face itself like this:\n",
    "    face_image = image[top:bottom, left:right]\n",
    "    pil_image = Image.fromarray(face_image)\n",
    "    pil_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function detectMultiScale:\n",
      "\n",
      "detectMultiScale(...) method of cv2.HOGDescriptor instance\n",
      "    detectMultiScale(img[, hitThreshold[, winStride[, padding[, scale[, finalThreshold[, useMeanshiftGrouping]]]]]]) -> foundLocations, foundWeights\n",
      "    .   @brief Detects objects of different sizes in the input image. The detected objects are returned as a list\n",
      "    .   of rectangles.\n",
      "    .   @param img Matrix of the type CV_8U or CV_8UC3 containing an image where objects are detected.\n",
      "    .   @param foundLocations Vector of rectangles where each rectangle contains the detected object.\n",
      "    .   @param foundWeights Vector that will contain confidence values for each detected object.\n",
      "    .   @param hitThreshold Threshold for the distance between features and SVM classifying plane.\n",
      "    .   Usually it is 0 and should be specified in the detector coefficients (as the last free coefficient).\n",
      "    .   But if the free coefficient is omitted (which is allowed), you can specify it manually here.\n",
      "    .   @param winStride Window stride. It must be a multiple of block stride.\n",
      "    .   @param padding Padding\n",
      "    .   @param scale Coefficient of the detection window increase.\n",
      "    .   @param finalThreshold Final threshold\n",
      "    .   @param useMeanshiftGrouping indicates grouping algorithm\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "help(cv2.HOGDescriptor().detectMultiScale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import datetime\n",
    "import imutils\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: __main__.py [-h] -i IMAGE [-w WIN_STRIDE] [-p PADDING] [-s SCALE]\n",
      "                   [-m MEAN_SHIFT]\n",
      "__main__.py: error: the following arguments are required: -i/--image\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-i\", \"--image\", required=True,\n",
    "\thelp=\"path to the input image\")\n",
    "ap.add_argument(\"-w\", \"--win-stride\", type=str, default=\"(8, 8)\",\n",
    "\thelp=\"window stride\")\n",
    "ap.add_argument(\"-p\", \"--padding\", type=str, default=\"(16, 16)\",\n",
    "\thelp=\"object padding\")\n",
    "ap.add_argument(\"-s\", \"--scale\", type=float, default=1.05,\n",
    "\thelp=\"image pyramid scale\")\n",
    "ap.add_argument(\"-m\", \"--mean-shift\", type=int, default=-1,\n",
    "\thelp=\"whether or not mean shift grouping should be used\")\n",
    "args = vars(ap.parse_args())\n",
    "\n",
    "# evaluate the command line arguments (using the eval function like\n",
    "# this is not good form, but let's tolerate it for the example)\n",
    "winStride = eval(args[\"win_stride\"])\n",
    "padding = eval(args[\"padding\"])\n",
    "meanShift = True if args[\"mean_shift\"] > 0 else False\n",
    " \n",
    "# initialize the HOG descriptor/person detector\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    " \n",
    "# load the image and resize it\n",
    "image = cv2.imread(args[\"image\"])\n",
    "image = imutils.resize(image, width=min(400, image.shape[1]))\n",
    "\n",
    "# detect people in the image\n",
    "start = datetime.datetime.now()\n",
    "(rects, weights) = hog.detectMultiScale(image, winStride=winStride,\n",
    "\tpadding=padding, scale=args[\"scale\"], useMeanshiftGrouping=meanShift)\n",
    "print(\"[INFO] detection took: {}s\".format(\n",
    "\t(datetime.datetime.now() - start).total_seconds()))\n",
    " \n",
    "# draw the original bounding boxes\n",
    "for (x, y, w, h) in rects:\n",
    "\tcv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    " \n",
    "# show the output image\n",
    "cv2.imshow(\"Detections\", image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'cv2' has no attribute 'SVM_C_SVC'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-10bc2275ad49>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m svm_params = dict( kernel_type = cv2.ml.SVM_LINEAR,\n\u001b[0;32m----> 8\u001b[0;31m                     \u001b[0msvm_type\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSVM_C_SVC\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m                     C=2.67, gamma=5.383 )\n\u001b[1;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'cv2' has no attribute 'SVM_C_SVC'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "SZ=20\n",
    "bin_n = 16 # Number of bins\n",
    "\n",
    "svm_params = dict( kernel_type = cv2.ml.SVM_LINEAR,\n",
    "                    svm_type = cv2.SVM_C_SVC,\n",
    "                    C=2.67, gamma=5.383 )\n",
    "\n",
    "affine_flags = cv2.WARP_INVERSE_MAP|cv2.INTER_LINEAR\n",
    "\n",
    "def deskew(img):\n",
    "    m = cv2.moments(img)\n",
    "    if abs(m['mu02']) < 1e-2:\n",
    "        return img.copy()\n",
    "    skew = m['mu11']/m['mu02']\n",
    "    M = np.float32([[1, skew, -0.5*SZ*skew], [0, 1, 0]])\n",
    "    img = cv2.warpAffine(img,M,(SZ, SZ),flags=affine_flags)\n",
    "    return img\n",
    "\n",
    "def hog(img):\n",
    "    gx = cv2.Sobel(img, cv2.CV_32F, 1, 0)\n",
    "    gy = cv2.Sobel(img, cv2.CV_32F, 0, 1)\n",
    "    mag, ang = cv2.cartToPolar(gx, gy)\n",
    "    bins = np.int32(bin_n*ang/(2*np.pi))    # quantizing binvalues in (0...16)\n",
    "    bin_cells = bins[:10,:10], bins[10:,:10], bins[:10,10:], bins[10:,10:]\n",
    "    mag_cells = mag[:10,:10], mag[10:,:10], mag[:10,10:], mag[10:,10:]\n",
    "    hists = [np.bincount(b.ravel(), m.ravel(), bin_n) for b, m in zip(bin_cells, mag_cells)]\n",
    "    hist = np.hstack(hists)     # hist is a 64 bit vector\n",
    "    return hist\n",
    "\n",
    "img = cv2.imread('face.jpg',0)\n",
    "\n",
    "cells = [np.hsplit(row,100) for row in np.vsplit(img,50)]\n",
    "\n",
    "# First half is trainData, remaining is testData\n",
    "train_cells = [ i[:50] for i in cells ]\n",
    "test_cells = [ i[50:] for i in cells]\n",
    "\n",
    "######     Now training      ########################\n",
    "\n",
    "deskewed = [map(deskew,row) for row in train_cells]\n",
    "hogdata = [map(hog,row) for row in deskewed]\n",
    "trainData = np.float32(hogdata).reshape(-1,64)\n",
    "responses = np.float32(np.repeat(np.arange(10),250)[:,np.newaxis])\n",
    "\n",
    "svm = cv2.SVM()\n",
    "svm.train(trainData,responses, params=svm_params)\n",
    "svm.save('svm_data.dat')\n",
    "\n",
    "######     Now testing      ########################\n",
    "\n",
    "deskewed = [map(deskew,row) for row in test_cells]\n",
    "hogdata = [map(hog,row) for row in deskewed]\n",
    "testData = np.float32(hogdata).reshape(-1,bin_n*4)\n",
    "result = svm.predict_all(testData)\n",
    "\n",
    "#######   Check Accuracy   ########################\n",
    "mask = result==responses\n",
    "correct = np.count_nonzero(mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
